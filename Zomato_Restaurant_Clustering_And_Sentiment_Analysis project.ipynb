{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"10B2s5af8INkHgIz_7XBjW3iNDbGY7ltx","timestamp":1731573910271}],"collapsed_sections":["vncDsAP0Gaoa","FJNUwmbgGyua","w6K7xa23Elo4","yQaldy8SH6Dl","mDgbUHAGgjLW","O_i_v8NEhb9l","HhfV-JJviCcP","Y3lxredqlCYt","3RnN4peoiCZX","x71ZqKXriCWQ","7hBIi_osiCS2","JlHwYmJAmNHm","35m5QtbWiB9F","PoPl-ycgm1ru","H0kj-8xxnORC","nA9Y7ga8ng1Z","PBTbrJXOngz2","u3PMJOP6ngxN","dauF4eBmngu3","bKJF3rekwFvQ","MSa1f5Uengrz","GF8Ens_Soomf","0wOQAZs5pc--","K5QZ13OEpz2H","lQ7QKXXCp7Bj","448CDAPjqfQr","KSlN3yHqYklG","t6dVpIINYklI","ijmpgYnKYklI","-JiQyfWJYklI","EM7whBJCYoAo","fge-S5ZAYoAp","85gYPyotYoAp","RoGjAbkUYoAp","4Of9eVA-YrdM","iky9q4vBYrdO","F6T5p64dYrdO","y-Ehk30pYrdP","bamQiAODYuh1","QHF8YVU7Yuh3","GwzvFGzlYuh3","qYpmQ266Yuh3","OH-pJp9IphqM","bbFf2-_FphqN","_ouA3fa0phqN","Seke61FWphqN","PIIx-8_IphqN","t27r6nlMphqO","r2jJGEOYphqO","b0JNsNcRphqO","BZR9WyysphqO","jj7wYXLtphqO","eZrbJ2SmphqO","rFu4xreNphqO","YJ55k-q6phqO","gCFgpxoyphqP","OVtJsKN_phqQ","lssrdh5qphqQ","U2RJ9gkRphqQ","1M8mcRywphqQ","tgIPom80phqQ","JMzcOPDDphqR","x-EpHcCOp1ci","X_VqEhTip1ck","8zGJKyg5p1ck","PVzmfK_Ep1ck","n3dbpmDWp1ck","ylSl6qgtp1ck","ZWILFDl5p1ck","M7G43BXep1ck","Ag9LCva-p1cl","E6MkPsBcp1cl","2cELzS2fp1cl","3MPXvC8up1cl","NC_X3p0fY2L0","UV0SzAkaZNRQ","YPEH6qLeZNRQ","q29F0dvdveiT","EXh0U9oCveiU","22aHeOlLveiV","g-ATYxFrGrvw","Yfr_Vlr8HBkt","8yEUt7NnHlrM","tEA2Xm5dHt1r","I79__PHVH19G","Ou-I18pAyIpj","fF3858GYyt-u","4_0_7-oCpUZd","hwyV_J3ipUZe","3yB-zSqbpUZe","dEUvejAfpUZe","Fd15vwWVpUZf","bn_IUdTipZyH","49K5P_iCpZyH","Nff-vKELpZyI","kLW572S8pZyI","dWbDXHzopZyI","yLjJCtPM0KBk","xiyOF9F70UgQ","7wuGOrhz0itI","id1riN9m0vUs","578E2V7j08f6","89xtkJwZ18nB","67NQN5KX2AMe","Iwf50b-R2tYG","GMQiZwjn3iu7","WVIkgGqN3qsr","XkPnILGE3zoT","Hlsf0x5436Go","mT9DMSJo4nBL","c49ITxTc407N","OeJFEK0N496M","9ExmJH0g5HBk","cJNqERVU536h","k5UmGsbsOxih","T0VqWOYE6DLQ","qBMux9mC6MCf","-oLEiFgy-5Pf","C74aWNz2AliB","2DejudWSA-a0","pEMng2IbBLp7","rAdphbQ9Bhjc","TNVZ9zx19K6k","nqoHp30x9hH9","rMDnDkt2B6du","yiiVWRdJDDil","1UUpS68QDMuG","kexQrXU-DjzY","T5CmagL3EC8N","BhH2vgX9EjGr","qjKvONjwE8ra","P1XJ9OREExlT","VFOzZv6IFROw","TIqpNgepFxVj","VfCC591jGiD4","OB4l2ZhMeS1U","ArJBuiUVfxKd","4qY1EAkEfxKe","PiV4Ypx8fxKe","TfvqoZmBfxKf","dJ2tPlVmpsJ0","JWYfwnehpsJ1","-jK_YjpMpsJ2","HAih1iBOpsJ2","zVGeBEFhpsJ2","bmKjuQ-FpsJ3","Fze-IPXLpx6K","7AN1z2sKpx6M","9PIHJqyupx6M","_-qAgymDpx6N","Z-hykwinpx6N","h_CCil-SKHpo","cBFFvTBNJzUa","HvGl1hHyA_VK","EyNgTHvd2WFk","KH5McJBi2d8v","iW_Lq9qf2h6X","-Kee-DAl2viO","gCX9965dhzqZ","gIfDvo9L0UH2"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Project Name**    -ZOMATO RESTAURANT CLUSTERING AND SENTIMENT ANALYSIS\n","\n"],"metadata":{"id":"vncDsAP0Gaoa"}},{"cell_type":"markdown","source":["##### **Project Type**    - Unsupervised\n","##### **Contribution**    - Individual"],"metadata":{"id":"beRrZCGUAJYm"}},{"cell_type":"markdown","source":["# **Project Summary -**"],"metadata":{"id":"FJNUwmbgGyua"}},{"cell_type":"markdown","source":["This project centers around Zomato, a prominent online food delivery platform. It involves two key datasets: one for restaurant information and the other for user reviews.\n","\n","The initial phase of this project involved rigorous data cleaning and preprocessing to ensure the data's suitability for comprehensive analysis. Subsequently, we conducted Exploratory Data Analysis (EDA) on both datasets, providing insights into the dataset's composition and features.\n","\n","The core analytical components of this project encompassed K-means clustering to group similar restaurants, and sentiment analysis of user reviews. To visually interpret the sentiment analysis results, we utilized an LDA visualizer. Furthermore, we compared the results with supervised methods to obtain a more profound understanding.\n","\n","In conclusion, this project offers a detailed exploration of Zomato's restaurant and review datasets, shedding light on clustering, sentiment analysis, and visualization to provide valuable insights for informed decision-making in the food industry."],"metadata":{"id":"F6v_1wHtG2nS"}},{"cell_type":"markdown","source":["# **GitHub Link -**"],"metadata":{"id":"w6K7xa23Elo4"}},{"cell_type":"markdown","source":["Provide your GitHub Link here."],"metadata":{"id":"h1o69JH3Eqqn"}},{"cell_type":"markdown","source":["# **Problem Statement**\n"],"metadata":{"id":"yQaldy8SH6Dl"}},{"cell_type":"markdown","source":["Project Title: Zomato Restaurant Clustering and Sentiment Analysis\n","\n","Objective: The objective of this project is to conduct EDA , Clustering and Sentiment Analysis on two datasets related to Zomato, a popular restaurant discovery and food delivery platform. The analysis aims to gain insights into restaurant clustering and user sentiment towards different restaurants listed on Zomato.\n","\n","Datasets\n","\n","Dataset 1: Zomato Restaurant's name and Metadata Details\n","\n","Description: This dataset contains details of various restaurants listed on Zomato. It includes features such as restaurant name, links ,costs, collections, cuisines, timings.\n","\n","Dataset 2: Zomato Restaurant Reviews\n","\n","Description: This dataset contains user reviews for different restaurants on Zomato. It includes information such as the text of the review, reviewer, ratings, metadata, time, pictures."],"metadata":{"id":"DpeJGUA3kjGy"}},{"cell_type":"markdown","source":["# **General Guidelines** : -  "],"metadata":{"id":"mDgbUHAGgjLW"}},{"cell_type":"markdown","source":["1.   Well-structured, formatted, and commented code is required.\n","2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n","     \n","     The additional credits will have advantages over other students during Star Student selection.\n","       \n","             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n","                       without a single error logged. ]\n","\n","3.   Each and every logic should have proper comments.\n","4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n","        \n","\n","```\n","# Chart visualization code\n","```\n","            \n","\n","*   Why did you pick the specific chart?\n","*   What is/are the insight(s) found from the chart?\n","* Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason.\n","\n","5. You have to create at least 15 logical & meaningful charts having important insights.\n","\n","\n","[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n","\n","U - Univariate Analysis,\n","\n","B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n","\n","M - Multivariate Analysis\n"," ]\n","\n","\n","\n","\n","\n","6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n","\n","\n","*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n","\n","\n","*   Cross- Validation & Hyperparameter Tuning\n","\n","*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n","\n","*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"ZrxVaUj-hHfC"}},{"cell_type":"markdown","source":["# ***Let's Begin !***"],"metadata":{"id":"O_i_v8NEhb9l"}},{"cell_type":"markdown","source":["## ***1. Know Your Data***"],"metadata":{"id":"HhfV-JJviCcP"}},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"id":"Y3lxredqlCYt"}},{"cell_type":"code","source":["# Import Libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.model_selection import GridSearchCV\n","from wordcloud import WordCloud\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"M8Vqi-pPk-HR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Loading"],"metadata":{"id":"3RnN4peoiCZX"}},{"cell_type":"code","source":["# Load Dataset\n","meta_df = pd.read_csv(\"/content/Zomato Restaurant names and Metadata.csv\")\n","review_df = pd.read_csv(\"/content/Zomato Restaurant reviews.csv\")"],"metadata":{"id":"4CkvbW_SlZ_R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset First View"],"metadata":{"id":"x71ZqKXriCWQ"}},{"cell_type":"code","source":["# Dataset First Look\n","meta_df.head()\n"],"metadata":{"id":"LWNFOSvLl09H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["review_df.head()"],"metadata":{"id":"sKZwSyH_4PKv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Dataset last view\n","\n","meta_df.tail()"],"metadata":{"id":"lGSIRAfB4T-z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["review_df.tail()"],"metadata":{"id":"Zxpam43V4cX0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Rows & Columns count"],"metadata":{"id":"7hBIi_osiCS2"}},{"cell_type":"code","source":["# Dataset Rows & Columns count\n","\n","print(\"Restaurants names and Metadata Observations: \", meta_df.shape)\n","print(\"Restaurant reviews: \", review_df.shape)"],"metadata":{"id":"Kllu7SJgmLij"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Information"],"metadata":{"id":"JlHwYmJAmNHm"}},{"cell_type":"code","source":["# Dataset Info\n","meta_df.info()\n","print()\n","review_df.info()"],"metadata":{"id":"e9hRXRi6meOf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Duplicate Values"],"metadata":{"id":"35m5QtbWiB9F"}},{"cell_type":"code","source":["# Dataset Duplicate Value Count\n","print(\"Duplicate values in Restaurant names and Metadata observations dataset: \", meta_df.duplicated().sum())\n","print(\"Duplicate values in Restaurant reviews dataset : \", review_df.duplicated().sum())"],"metadata":{"id":"1sLdpKYkmox0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Missing Values/Null Values"],"metadata":{"id":"PoPl-ycgm1ru"}},{"cell_type":"code","source":["# Missing Values/Null Values Count\n","\n","print(meta_df.isnull().sum())\n","print()\n","print(review_df.isnull().sum())"],"metadata":{"id":"GgHWkxvamxVg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing the missing values\n","\n","#Visualise missing values in meta_Df dataset\n","plt.figure(figsize=(6,6))\n","sns.heatmap(meta_df.isnull(), cbar=False, cmap='viridis')\n","plt.title('Missing values in meta_df')\n","plt.show()"],"metadata":{"id":"3q5wnI3om9sJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize missing values in the review_df dataset\n","\n","plt.figure(figsize=(6,6))\n","sns.heatmap(review_df.isnull(), cbar=False, cmap='viridis')\n","plt.title('Missing values in review_df')\n","plt.show()"],"metadata":{"id":"Z4rM2aAk6VWX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What did you know about your dataset?"],"metadata":{"id":"H0kj-8xxnORC"}},{"cell_type":"markdown","source":["Restaurant Dataset(meta_df)\n","\n","\n","*   it contains 105 rows and 6 features\n","*   Collection and Timing features has null values\n","*   No duplicated values\n","*   Feature cost and timings are of object datatype\n","\n","Review Dataset(review_df)\n","\n","*   It contains 10000 rows and 7 features\n","*   All features have null values except picture and restaurant\n","*   36 duplicated rows\n","*   Rating and Time are of object datatype.\n","\n","\n","\n"],"metadata":{"id":"gfoNAAC-nUe_"}},{"cell_type":"markdown","source":["## ***2. Understanding Your Variables***"],"metadata":{"id":"nA9Y7ga8ng1Z"}},{"cell_type":"code","source":["# Dataset Columns\n","\n","print(meta_df.columns)\n","print(review_df.columns)"],"metadata":{"id":"j7xfkqrt5Ag5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataset Describe\n","meta_df.describe(include = 'all')"],"metadata":{"id":"DnOaZdaE5Q5t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["review_df.describe(include = 'all')"],"metadata":{"id":"aPZ4ot9Y7uEJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Variables Description"],"metadata":{"id":"PBTbrJXOngz2"}},{"cell_type":"markdown","source":["Restaurant Datset\n","\n","\n","*   Name : Name of Restaurant\n","*   Links : URL Links of restaurant\n","*   Cost : per person estimated cost of dining\n","*   Collection : Tagging of Restaurants w.r.t Zomato categories\n","*   Cuisines : Cuisiness served by Restaurant\n","*   Timings : Restaurant timings\n","\n","Review Dataset:\n","\n","\n","1.   Restaurant : Name of the restaurant\n","2.   Reviewer : Name of the reviewer\n","3.   Review : Review text\n","4.   Metadata : Reviewer Metadata-No. of reviews and followers\n","5.   Time : Date and time of review\n","6.   Pictures : No. of pictures with review.\n","\n","\n","\n","\n"],"metadata":{"id":"aJV4KIxSnxay"}},{"cell_type":"markdown","source":["### Check Unique Values for each variable."],"metadata":{"id":"u3PMJOP6ngxN"}},{"cell_type":"code","source":["# Check Unique Values for each variable.\n","print(\"Meta dataset:\\n\",meta_df.nunique())\n","print()\n","print(\"Review dataset:\\n\", review_df.nunique())"],"metadata":{"id":"zms12Yq5n-jE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. ***Data Wrangling***"],"metadata":{"id":"dauF4eBmngu3"}},{"cell_type":"markdown","source":["### Data Wrangling Code"],"metadata":{"id":"bKJF3rekwFvQ"}},{"cell_type":"code","source":["# Write your code to make your dataset analysis ready.\n","meta_df['Cost'] = meta_df['Cost'].str.replace(',','').astype('int64')"],"metadata":{"id":"wk-9a2fpoLcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# removing null values\n","review_df = review_df.dropna()"],"metadata":{"id":"GSa3N2UA9kW7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#converting rating to float\n","review_df.drop(review_df[(review_df['Rating']=='Like')].index, inplace=True)\n","review_df['Rating']= review_df['Rating'].astype('float64')\n","review_df.shape"],"metadata":{"id":"PCIqev999sv6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Extracting details from metadata\n","\n","review_df['Reviewer_Total_Review']=review_df['Metadata'].str.split(',').str[0]\n","review_df['Reviewer_followers']= review_df['Metadata'].str.split(',').str[1]\n","review_df['Reviewer_Total_Review']=pd.to_numeric(review_df['Reviewer_Total_Review'].str.split(' ').str[0])\n","review_df['Reviewer_followers']=pd.to_numeric(review_df['Reviewer_followers'].str.split(' ').str[1])"],"metadata":{"id":"Tgbc0PpV_CTD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["review_df.head()"],"metadata":{"id":"4860F2kKAy-W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Extracting review time, year, month and Hour for analysis\n","\n","review_df['Time']=pd.to_datetime(review_df['Time'])\n","review_df['Review_Year'] = pd.DatetimeIndex(review_df['Time']).year\n","review_df['Review_Month']= pd.DatetimeIndex(review_df['Time']).month\n","review_df['Review_Hour'] = pd.DatetimeIndex(review_df['Time']).hour"],"metadata":{"id":"_2eEpyb9A5Sm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What all manipulations have you done and insights you found?"],"metadata":{"id":"MSa1f5Uengrz"}},{"cell_type":"markdown","source":["Restaurant Data:\n","\n","I have converted the data type of the 'cost' feature from object to integer for improved consistency and numeric analysis\n","\n","Review Data:\n","\n","*   Since there were very few missing values in the review data, i opted to remove them to maintain data integrity\n","*   Additionally, i enhanced the dataset by converting the 'rating' feature from object to float for precise analysis.\n","*   I enriched the dataset by extracting valuable information such as 'followers' and 'reviews' from the metadata\n","*   To provide a more comprehensive analysis, i also extracted the 'Time', 'Month', 'Year' and 'Hour'\n","\n"],"metadata":{"id":"LbyXE7I1olp8"}},{"cell_type":"markdown","source":["## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"],"metadata":{"id":"GF8Ens_Soomf"}},{"cell_type":"markdown","source":["#### Chart - 1"],"metadata":{"id":"0wOQAZs5pc--"}},{"cell_type":"code","source":["# Chart - 1 visualization code\n","# Costlier and affordable restaurants\n","# Sort the data by 'Cost' in descending order for costlier restaurants\n","costlier_data = meta_df[['Name', 'Cost']].sort_values(by='Cost', ascending=False).head(10)\n","\n","# Sort the data by 'Cost' in ascending order for affordable restaurants\n","affordable_data = meta_df[['Name', 'Cost']].sort_values(by='Cost', ascending=True).head(10)\n","\n","# Create subplots\n","fig, axes = plt.subplots(1, 2, figsize=(30, 10))\n","\n","# Plot costlier restaurants\n","sns.barplot(data=costlier_data, x='Cost', y='Name', ax=axes[0],palette='cubehelix')\n","axes[0].set_title(\"Top 10 Costlier Restaurants by Cost\")\n","\n","# Plot affordable restaurants\n","sns.barplot(data=affordable_data, x='Cost', y='Name', ax=axes[1],palette='Accent')\n","axes[1].set_title(\"Top 10 Affordable Restaurants by Cost\")\n","\n","plt.show()"],"metadata":{"id":"7v_ESjsspbW7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"K5QZ13OEpz2H"}},{"cell_type":"markdown","source":["To identify the more expensive restaurants.\n"],"metadata":{"id":"XESiWehPqBRc"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"lQ7QKXXCp7Bj"}},{"cell_type":"markdown","source":["Among all the restaurants, Collage at Hyatt Hyderabad Gachibowli is the priciest."],"metadata":{"id":"C_j1G7yiqdRP"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"448CDAPjqfQr"}},{"cell_type":"markdown","source":["We can recommend this restaurant to customers who are looking for upscale dining experiences with higher-priced menu items"],"metadata":{"id":"3cspy4FjqxJW"}},{"cell_type":"markdown","source":["#### Chart - 2"],"metadata":{"id":"KSlN3yHqYklG"}},{"cell_type":"code","source":["# Chart - 2 visualization code\n","# Top 10 Cuisines by Restaurant count\n","# Splitting all cuisines to list\n","cuisine_list = meta_df['Cuisines'].str.split(', ').apply(lambda x : [word for word in x] )"],"metadata":{"id":"R4YgtaqtYklH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cuisine_top_dict = {}\n","for x in cuisine_list:\n","  for cuisine in x:\n","    if cuisine in cuisine_top_dict:\n","      cuisine_top_dict[cuisine] += 1\n","    else :\n","      cuisine_top_dict[cuisine] = 1\n","cuisine_top_df = pd.DataFrame(((k,vals) for k,vals in cuisine_top_dict.items()),columns=['Cuisine','Restaurant count']).sort_values(by= 'Restaurant count' ,ascending=False).head(10)"],"metadata":{"id":"hgeOIgW0E25a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.barplot(x = cuisine_top_df['Cuisine'], y = cuisine_top_df['Restaurant count'], palette='cubehelix')\n","plt.title(\"Top 10 Cuisines by Restaurant Count\")\n","plt.xticks(rotation=90)"],"metadata":{"id":"DR1EOYn9FJDM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t6dVpIINYklI"}},{"cell_type":"markdown","source":["\n","It presents a variety of cuisines offered by restaurants in terms of count."],"metadata":{"id":"5aaW0BYyYklI"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"ijmpgYnKYklI"}},{"cell_type":"markdown","source":["North Indian restaurants are prevalent, primarily due to a significant number of patrons originating from the North Indian region."],"metadata":{"id":"PSx9atu2YklI"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"-JiQyfWJYklI"}},{"cell_type":"markdown","source":["As evident from the data, there is a high demand for North Indian and Chinese cuisine. Therefore, it makes strategic sense to focus on targeting these customer segments."],"metadata":{"id":"BcBbebzrYklV"}},{"cell_type":"markdown","source":["#### Chart - 3"],"metadata":{"id":"EM7whBJCYoAo"}},{"cell_type":"code","source":["# Chart - 3 visualization code\n","# Group the data by 'Reviewer' and find the maximum 'Reviewer_Followers' for each reviewer\n","max_followers_by_reviewer = review_df.groupby('Reviewer')['Reviewer_followers'].max()\n","\n","# Sort the results in descending order and select the top 10\n","top_10_reviewers = max_followers_by_reviewer.sort_values(ascending=False).head(10)\n","\n","# Create a bar plot\n","top_10_reviewers.plot(kind='bar')\n","\n","# Set the plot title and labels if needed\n","plt.title(\"Top 10 Reviewers by Max Followers\")\n","plt.xlabel(\"Reviewers\")\n","plt.ylabel(\"Max Followers\")"],"metadata":{"id":"t6GMdE67YoAp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"fge-S5ZAYoAp"}},{"cell_type":"markdown","source":["These individuals have the highest number of followers, indicating that their reviews have the potential to influence a larger audience."],"metadata":{"id":"5dBItgRVYoAp"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"85gYPyotYoAp"}},{"cell_type":"markdown","source":["We've discovered that Satwinder Singh and Eat_with_me are the top two reviewers with the largest number of followers."],"metadata":{"id":"4jstXR6OYoAp"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"RoGjAbkUYoAp"}},{"cell_type":"markdown","source":["If we require advertising services, these individuals could be potential choices."],"metadata":{"id":"zfJ8IqMcYoAp"}},{"cell_type":"markdown","source":["#### Chart - 4"],"metadata":{"id":"4Of9eVA-YrdM"}},{"cell_type":"code","source":["# Chart - 4 visualization code\n","\n","collection_list = meta_df['Collections'].dropna().apply(lambda x: [collection.strip() for collection in x.split(', ')])\n","collection_top_dict = {}\n","for x in collection_list:\n","    for collection in x:\n","        if collection in collection_top_dict:\n","            collection_top_dict[collection] += 1\n","        else:\n","            collection_top_dict[collection] = 1\n","collection_top_df = pd.DataFrame(((k, vals) for k, vals in collection_top_dict.items()), columns=['Collection', 'Restaurant count']).sort_values(by='Restaurant count', ascending=False)\n","\n","# Plot the top collections\n","plt.figure(figsize=(6, 6))  # Adjust the figure size as needed\n","ax = sns.barplot(y=collection_top_df['Collection'].head(10), x=collection_top_df['Restaurant count'].head(10), palette='viridis')\n","ax.set_title('Top 10 Restaurant Collections')\n","plt.xlabel('Restaurant Count')\n","plt.ylabel('Collection')\n","plt.title('Top 10 Restaurant Collections')  # Add your title here\n","\n","plt.show()"],"metadata":{"id":"irlUoxc8YrdO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"iky9q4vBYrdO"}},{"cell_type":"markdown","source":["The chart provides insight into the most popular restaurant tags according to Zomato categories."],"metadata":{"id":"aJRCwT6DYrdO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"F6T5p64dYrdO"}},{"cell_type":"markdown","source":["Great Buffets is mostly used tag restaurant."],"metadata":{"id":"Xx8WAJvtYrdO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"y-Ehk30pYrdP"}},{"cell_type":"markdown","source":["Insights aid in identifying the most popular restaurant tags, enabling businesses to offer promotions and discounts to those with lower popularity."],"metadata":{"id":"jLNxxz7MYrdP"}},{"cell_type":"markdown","source":["#### Chart - 5"],"metadata":{"id":"bamQiAODYuh1"}},{"cell_type":"code","source":["# Chart - 5 visualization code\n","\n","# Group the data by 'Reviewer', calculate the mean 'Rating' and maximum 'Reviewer_Followers'\n","reviewer_stats = review_df.groupby('Reviewer')[['Rating', 'Reviewer_followers']].mean()\n","\n","# Sort the results by 'Reviewer_Followers' in descending order and select the top 10\n","top_10_reviewers_stats = reviewer_stats.sort_values(by=['Reviewer_followers'], ascending=False).head(10)\n","\n","# Drop the 'Reviewer_Followers' column as it's not needed in the plot\n","top_10_reviewers_stats.drop(['Reviewer_followers'], axis=1, inplace=True)\n","\n","# Create a bar plot\n","top_10_reviewers_stats.plot(kind='bar')\n","\n","# Set the plot title\n","plt.title(\"Top 10 Reviewers by Average Rating\")\n","\n","# Set labels if needed\n","plt.xlabel(\"Reviewers\")\n","plt.ylabel(\"Average Rating\")\n","\n","# Show the plot\n","plt.show()"],"metadata":{"id":"TIJwrbroYuh3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"QHF8YVU7Yuh3"}},{"cell_type":"markdown","source":["Understanding the average ratings of the most-followed reviewers is essential for assessing their potential biases."],"metadata":{"id":"dcxuIMRPYuh3"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"GwzvFGzlYuh3"}},{"cell_type":"markdown","source":["For instance, reviewers like 'eat_vth_me' and 'foodporn' have consistently given an average rating of 5. This could imply either frequent visits to top-notch restaurants or a tendency to rate all restaurants with a perfect score."],"metadata":{"id":"uyqkiB8YYuh3"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"qYpmQ266Yuh3"}},{"cell_type":"markdown","source":["Such insights given in charts are valuable for sentiment analysis and provide essential context for restaurant reviews."],"metadata":{"id":"_WtzZ_hCYuh4"}},{"cell_type":"markdown","source":["#### Chart - 6"],"metadata":{"id":"OH-pJp9IphqM"}},{"cell_type":"code","source":["# Chart - 6 visualization code\n","# Create a bar chart for Review_Month\n","review_month_counts = review_df['Review_Month'].value_counts()\n","\n","# Set plot style\n","sns.set(style=\"whitegrid\")\n","\n","# Create the bar plot\n","plt.figure(figsize=(12, 6))\n","sns.barplot(x=review_month_counts.index, y=review_month_counts.values, palette=\"viridis\")\n","\n","# Set x and y labels\n","plt.xlabel(\"Review Month\")\n","plt.ylabel(\"Count\")\n","\n","# Set the title\n","plt.title(\"Review Counts by Month\")\n","\n","# Show the plot\n","plt.show()"],"metadata":{"id":"kuRf4wtuphqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"bbFf2-_FphqN"}},{"cell_type":"markdown","source":["To understand the peak engagement periods for restaurants."],"metadata":{"id":"loh7H2nzphqN"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"_ouA3fa0phqN"}},{"cell_type":"markdown","source":["Feb to Aug there is very active period for restaurants except June."],"metadata":{"id":"VECbqPI7phqN"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"Seke61FWphqN"}},{"cell_type":"markdown","source":["During these time frames, Zomato may consider increasing their delivery personnel and offering promotional codes to boost activity in less active periods"],"metadata":{"id":"DW4_bGpfphqN"}},{"cell_type":"markdown","source":["#### Chart - 7"],"metadata":{"id":"PIIx-8_IphqN"}},{"cell_type":"code","source":["# Chart - 7 visualization code\n","# Combine the top 30 reviews into a single text\n","top_reviews_text = \" \".join(name for name in review_df.sort_values('Review', ascending=False).Review[:30])\n","\n","# Create a WordCloud with specified parameters\n","word_cloud = WordCloud(\n","    width=1400,\n","    height=1400,\n","    collocations=False,\n","    background_color='white'\n",").generate(top_reviews_text)\n","\n","# Set up the plot\n","plt.figure(figsize=(15, 8))\n","plt.imshow(word_cloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","\n","# Display the Word Cloud\n","plt.show()"],"metadata":{"id":"lqAIGUfyphqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t27r6nlMphqO"}},{"cell_type":"markdown","source":["The Word Cloud chart was chosen for its ability to visually represent the most frequent words in the top 30 reviews, offering a concise summary of prominent themes and sentiments."],"metadata":{"id":"iv6ro40sphqO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"r2jJGEOYphqO"}},{"cell_type":"markdown","source":["The terms 'food,' 'good,' and 'place' are the most commonly occurring words in the reviews."],"metadata":{"id":"Po6ZPi4hphqO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"b0JNsNcRphqO"}},{"cell_type":"markdown","source":["The insights gained from the Word Cloud can be valuable for creating a positive business impact. If the prevalent words in the Word Cloud reflect positive experiences, it can inform the restaurant's strengths and areas where they excel."],"metadata":{"id":"xvSq8iUTphqO"}},{"cell_type":"markdown","source":["#### Chart - 8"],"metadata":{"id":"BZR9WyysphqO"}},{"cell_type":"code","source":["# Chart - 8 visualization code\n","# Get the top 5 reviewers by review count\n","top_reviewers = review_df['Reviewer'].value_counts().head(5)\n","\n","# Create a bar plot\n","top_reviewers.plot(kind='bar')\n","\n","# Set the plot title and labels if needed\n","plt.title(\"Top 5 Reviewers by Review Count\")\n","plt.xlabel(\"Reviewers\")\n","plt.ylabel(\"Review Count\")\n","\n","# Show the plot\n","plt.show()\n"],"metadata":{"id":"TdPTWpAVphqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"jj7wYXLtphqO"}},{"cell_type":"markdown","source":["To identify the reviewers who are most active."],"metadata":{"id":"Ob8u6rCTphqO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"eZrbJ2SmphqO"}},{"cell_type":"markdown","source":["We have determined that Ankita, Parijat, and Kiran are the top three reviewers based on their activity."],"metadata":{"id":"mZtgC_hjphqO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"rFu4xreNphqO"}},{"cell_type":"markdown","source":["We can consider engaging Ankita, Parijat, and Kiran as reviewers for multiple restaurants. Given their prolific reviewing activity, their preferences align with the type of restaurants they may favor.\n","\n"],"metadata":{"id":"ey_0qi68phqO"}},{"cell_type":"markdown","source":["#### Chart - 9"],"metadata":{"id":"YJ55k-q6phqO"}},{"cell_type":"code","source":["# Chart - 9 visualization code\n","# Calculate the average cost per cuisine\n","cuisine_cost = meta_df['Cuisines'].str.split(', ').explode().to_frame(name='Cuisine')\n","cuisine_cost['Cost'] = meta_df['Cost'] / meta_df['Cuisines'].str.count(', ') + 1\n","\n","# Group and calculate the average cost per cuisine\n","cuisine_avg_cost = cuisine_cost.groupby('Cuisine')['Cost'].mean()\n","\n","# Exclude Lebanese cuisine\n","cuisine_avg_cost = cuisine_avg_cost[cuisine_avg_cost.index != 'Lebanese']\n","\n","# Select the top 10 cuisines\n","top_10_cuisines = cuisine_avg_cost.nlargest(10)\n","\n","# Create a bar plot for the top 10 cuisines\n","plt.figure(figsize=(12, 6))\n","top_10_cuisines.plot(kind='bar', color='orange')\n","plt.title('Top 10 Cuisines by Average Cost ')\n","plt.xlabel('Cuisine')\n","plt.ylabel('Average Cost')\n","plt.xticks(rotation=90)\n","plt.show()"],"metadata":{"id":"B2aS4O1ophqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"gCFgpxoyphqP"}},{"cell_type":"markdown","source":["It reveals the cuisine with the highest average cost from the given list."],"metadata":{"id":"TVxDimi2phqP"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"OVtJsKN_phqQ"}},{"cell_type":"markdown","source":["I've identified that 'Modern Indian,' 'Sushi,' and 'BBQ' cuisines rank among the most expensive options in terms of cost"],"metadata":{"id":"ngGi97qjphqQ"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"lssrdh5qphqQ"}},{"cell_type":"markdown","source":["Yes, Restaurant owners or operators can leverage this information to make strategic decisions, such as pricing these cuisines competitively and tailoring marketing efforts to attract customers seeking premium dining experiences. By catering to the demand for these higher-cost cuisines, the business can potentially increase its revenue and profitability."],"metadata":{"id":"tBpY5ekJphqQ"}},{"cell_type":"markdown","source":["#### Chart - 10"],"metadata":{"id":"U2RJ9gkRphqQ"}},{"cell_type":"code","source":["# Chart - 10 Correlation Heatmap\n","\n","# Correlation Heatmap visualization code\n","# Heatmap of review_df\n","meta = meta_df.rename(columns = {'Name':'Restaurant'})\n","merged = meta.merge(review_df, on = 'Restaurant')\n","merged.shape"],"metadata":{"id":"GM7a4YP4phqQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a correlation matrix for the desired columns\n","correlation_matrix = merged[merged.describe().columns].corr()\n","\n","# Set the figure size\n","fig, ax = plt.subplots(figsize=(20, 10))\n","\n","# Create a heatmap with annotations\n","sns.heatmap(correlation_matrix, ax=ax, annot=True, cmap='rocket', linewidths=1)\n","\n","# Show the heatmap\n","plt.show()"],"metadata":{"id":"q5L73tQfIxJk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"1M8mcRywphqQ"}},{"cell_type":"markdown","source":["To assess the correlation among all numeric variables."],"metadata":{"id":"8agQvks0phqQ"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"tgIPom80phqQ"}},{"cell_type":"markdown","source":["The dataset contains limited data for certain years, such as 2018 and 2019, with particularly sparse data for the years 2016 and 2017. Consequently, the correlation analysis may not yield significant insights."],"metadata":{"id":"Qp13pnNzphqQ"}},{"cell_type":"markdown","source":["## ***5. Hypothesis Testing***"],"metadata":{"id":"g-ATYxFrGrvw"}},{"cell_type":"markdown","source":["### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."],"metadata":{"id":"Yfr_Vlr8HBkt"}},{"cell_type":"markdown","source":["\n","\n","*   Restaurants with higher ratings tend to have higher prices.\n","*   The rating of a restaurant doesn't seem to be influenced by the number of followers its most-followed reviewer has.\n","*  Restaurants offering a greater variety of cuisines may receive higher ratings."],"metadata":{"id":"-7MS06SUHkB-"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 1"],"metadata":{"id":"8yEUt7NnHlrM"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"tEA2Xm5dHt1r"}},{"cell_type":"markdown","source":["\n","\n","1.   Null Hypothesis : No correlation between rating and cost.\n","2.   Alternate Hypothesis : There is a correlation between them\n","3.   Test : Simple Linear Regression analysis\n","\n","\n","\n"],"metadata":{"id":"HI9ZP0laH0D-"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"I79__PHVH19G"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","import statsmodels.formula.api as sm\n","model = sm.ols(formula='Rating ~ Cost', data= merged).fit()\n","p_value = model.pvalues[1]\n","if p_value < 0.05 :\n","  print('Null Hypothesis is rejected')\n","else:\n","  print('Fail to reject Null Hypothesis')"],"metadata":{"id":"oZrfquKtyian"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"Ou-I18pAyIpj"}},{"cell_type":"markdown","source":["The statistical test conducted to obtain the p-value is simple linear regression."],"metadata":{"id":"s2U0kk00ygSB"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"fF3858GYyt-u"}},{"cell_type":"markdown","source":["Linear regression was chosen as it assesses the relationship between 'Rating' and 'Cost,' helping determine if cost significantly impacts restaurant ratings"],"metadata":{"id":"HO4K0gP5y3B4"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 2"],"metadata":{"id":"4_0_7-oCpUZd"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"hwyV_J3ipUZe"}},{"cell_type":"markdown","source":["\n","\n","*  Null Hypothesis : The number of followers and reviewer has has no effect on the rating of a restaurant.\n","  \n","*   Alternate Hypothesis : There is a positive relation between them.\n","\n"],"metadata":{"id":"FnpLGJ-4pUZe"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"3yB-zSqbpUZe"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","import statsmodels.formula.api as sm\n","model = sm.ols(formula='Rating ~ Reviewer_followers', data = merged).fit()\n","p_value = model.pvalues[1]\n","if p_value < 0.05 :\n","  print('Null Hypothesis is rejected')\n","else:\n","  print('Fail to reject Null Hypothesis')"],"metadata":{"id":"sWxdNTXNpUZe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"dEUvejAfpUZe"}},{"cell_type":"markdown","source":["\n","The statistical test performed to obtain the P-value is linear regression using the OLS (Ordinary Least Squares) method."],"metadata":{"id":"oLDrPz7HpUZf"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"Fd15vwWVpUZf"}},{"cell_type":"markdown","source":["Linear regression is appropriate when you want to investigate the relationship between two continuous variables, which is the case here with \"Rating\" and \"Reviewer_Followers.\"\n","\n","Linear regression assumes a linear relationship between the variables, which is a reasonable assumption in this context."],"metadata":{"id":"4xOGYyiBpUZf"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 3"],"metadata":{"id":"bn_IUdTipZyH"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"49K5P_iCpZyH"}},{"cell_type":"markdown","source":["\n","\n","*   Null Hypothesis : Restaurants cuisines has no relation with the rating.\n","*   Alternate Hypothesis : There is a relation between them.\n","*   Test : Chi2\n","\n"],"metadata":{"id":"7gWI5rT9pZyH"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"Nff-vKELpZyI"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","from scipy.stats import chi2_contingency\n","ct = pd.crosstab(merged['Cuisines'], merged['Rating'])\n","chi2, p, dof, expected = chi2_contingency(ct)\n","if p < 0.05:\n","    print(\"Reject Null Hypothesis\")\n","else:\n","    print(\"Fail to reject Null Hypothesis\")"],"metadata":{"id":"s6AnJQjtpZyI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"kLW572S8pZyI"}},{"cell_type":"markdown","source":["The statistical test performed to obtain the P-value is the Chi-squared test of independence."],"metadata":{"id":"ytWJ8v15pZyI"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"dWbDXHzopZyI"}},{"cell_type":"markdown","source":["It is appropriate for analyzing the relationship between two categorical variables, which are \"Cuisines\" and \"Rating\" in this case.\n","\n","The test allows for hypothesis testing to determine whether there is a statistically significant association between the variables, making it a suitable choice for this analysis."],"metadata":{"id":"M99G98V6pZyI"}},{"cell_type":"markdown","source":["## ***6. Feature Engineering & Data Pre-processing***"],"metadata":{"id":"yLjJCtPM0KBk"}},{"cell_type":"markdown","source":["### 1. Handling Missing Values"],"metadata":{"id":"xiyOF9F70UgQ"}},{"cell_type":"code","source":["# Handling Missing Values & Missing Value Imputation\n","#Checking the mean rating given by all to impute them in missing values of collection\n","round(collection_top_df['Restaurant count'].mean(),0)"],"metadata":{"id":"iRsAHk1K0fpS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# collection_top_df"],"metadata":{"id":"OHn9uHzIMBxw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["collection_top_df[collection_top_df['Restaurant count']==3]['Collection'].tolist()"],"metadata":{"id":"zSLOOUgVMDJB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mean_collection = \", \".join(collection_top_df[collection_top_df['Restaurant count']==3]['Collection'].tolist())\n","meta_df['Collections'].fillna(mean_collection,inplace=True)\n","meta_df.dropna(inplace=True)"],"metadata":{"id":"o1ybL_vnMJQz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["review_df['Reviewer_followers'].fillna(0,inplace=True)"],"metadata":{"id":"0OOhZAb5MSTI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(meta_df.isna().sum())\n","print(review_df.isna().sum())"],"metadata":{"id":"p8DoOlp8MYfA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(meta_df.shape)\n","print(review_df.shape)"],"metadata":{"id":"txHHy0WHMdUy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What all missing value imputation techniques have you used and why did you use those techniques?"],"metadata":{"id":"7wuGOrhz0itI"}},{"cell_type":"markdown","source":["\n","\n","*   I filled in missing collection values with the mean value of 3, which includes 'Barbecue & Grill,' 'Happy Hours,' and 'Gigs and Events.'\n","*   I replaced NaN values in the number of followers with 0."],"metadata":{"id":"1ixusLtI0pqI"}},{"cell_type":"markdown","source":["### 2. Handling Outliers"],"metadata":{"id":"id1riN9m0vUs"}},{"cell_type":"code","source":["# Handling Outliers & Outlier treatments\n"],"metadata":{"id":"M6w2CzZf04JK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What all outlier treatment techniques have you used and why did you use those techniques?"],"metadata":{"id":"578E2V7j08f6"}},{"cell_type":"markdown","source":["As data is very less will not do outlier detection"],"metadata":{"id":"uGZz5OrT1HH-"}},{"cell_type":"markdown","source":["### 3. Categorical Encoding"],"metadata":{"id":"89xtkJwZ18nB"}},{"cell_type":"code","source":["\n","meta_df.rename(columns={'Name':'Restaurant'},inplace=True)"],"metadata":{"id":"21JmIYMG2hEo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Encode your categorical columns\n","cluster_dummy = meta_df[['Restaurant','Cuisines']]\n","cluster_dummy['Cuisines'] = cluster_dummy['Cuisines'].str.split(',')\n","cluster_dummy = cluster_dummy.explode('Cuisines')\n","cluster_dummy['Cuisines'] = cluster_dummy['Cuisines'].apply(lambda x: x.strip())\n","cluster_dummy = pd.get_dummies(cluster_dummy, columns=[\"Cuisines\"], prefix=[\"Cuisines\"])\n","cluster_dummy.loc[:, cluster_dummy.columns.str.startswith('Cuisines_')].idxmax(1)[:6]\n","cluster_dummy.columns = cluster_dummy.columns.str.replace(\"Cuisines_\",\"\")\n","cluster_dummy = cluster_dummy.groupby(\"Restaurant\").sum().reset_index()"],"metadata":{"id":"WiP5xog8M6-x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["meta_df['Total_Cuisine_Count'] = meta_df['Cuisines'].apply(lambda x : len(x.split(',')))"],"metadata":{"id":"Yzr4qYvwNGjj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["avg_hotel_rating = review_df.groupby('Restaurant').agg({'Rating':'mean',\n","        'Reviewer': 'count'}).reset_index().rename(columns = {'Reviewer': 'Total_Review'})"],"metadata":{"id":"WVD62H1DNPCd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["avg_hotel_rating.rename(columns = {'Rating':'Average_Rating'}, inplace =True)\n","meta_df = meta_df.merge(avg_hotel_rating[['Average_Rating','Restaurant']], on = 'Restaurant')\n","meta_df.head(1)"],"metadata":{"id":"yErYFRohNSim"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cluster_dummy = meta_df[['Restaurant','Cost','Average_Rating','Total_Cuisine_Count']].merge(cluster_dummy, on = 'Restaurant')"],"metadata":{"id":"byfWQMShNahB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cluster_dummy.head(1)"],"metadata":{"id":"t5IpT_-BNesJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What all categorical encoding techniques have you used & why did you use those techniques?"],"metadata":{"id":"67NQN5KX2AMe"}},{"cell_type":"markdown","source":["I performed one-hot encoding on the cuisines column while leaving the average rating and cost variables unchanged since they are numerical variables."],"metadata":{"id":"UDaue5h32n_G"}},{"cell_type":"markdown","source":["### 4. Textual Data Preprocessing\n","(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"],"metadata":{"id":"Iwf50b-R2tYG"}},{"cell_type":"markdown","source":["#### 1. Expand Contraction"],"metadata":{"id":"GMQiZwjn3iu7"}},{"cell_type":"code","source":["# Expand Contraction\n","sentiment_df = review_df[['Reviewer','Restaurant','Rating','Review']]\n","sentiment_df = sentiment_df.reset_index()\n","sentiment_df['index'] = sentiment_df.index"],"metadata":{"id":"PTouz10C3oNN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install contractions"],"metadata":{"id":"dgmxA9QDNwgb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import contractions\n","sentiment_df['Review']=sentiment_df['Review'].apply(lambda x:contractions.fix(x))"],"metadata":{"id":"UKfw5SKWN53_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Lower Casing"],"metadata":{"id":"WVIkgGqN3qsr"}},{"cell_type":"code","source":["# Lower Casing\n","sentiment_df['Review'] = sentiment_df['Review'].str.lower()"],"metadata":{"id":"88JnJ1jN3w7j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3. Removing Punctuations"],"metadata":{"id":"XkPnILGE3zoT"}},{"cell_type":"code","source":["# Remove Punctuations\n","import string\n","def remove_punctuation(text):\n","  translator = str.maketrans('', '', string.punctuation)\n","  return text.translate(translator)"],"metadata":{"id":"vqbBqNaA33c0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentiment_df['Review'] = sentiment_df['Review'].apply(remove_punctuation)"],"metadata":{"id":"GXhLM9X0OLhE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 4. Removing URLs & Removing words and digits contain digits."],"metadata":{"id":"Hlsf0x5436Go"}},{"cell_type":"code","source":["# Remove URLs & Remove words and digits contain digits\n","import re\n","sentiment_df[\"Review\"] = sentiment_df[\"Review\"].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n","sentiment_df[\"Review\"] = sentiment_df[\"Review\"].apply(lambda x: re.sub(r\"\\d+\", \"\", x))"],"metadata":{"id":"2sxKgKxu4Ip3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 5. Removing Stopwords & Removing White spaces"],"metadata":{"id":"mT9DMSJo4nBL"}},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')"],"metadata":{"id":"ojMqQVo3OZvq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove Stopwords\n","sw = stopwords.words('english')\n","def delete_stopwords(text):\n","  text = [word.lower() for word in text.split() if word.lower() not in sw]\n","  return \" \".join(text)"],"metadata":{"id":"T2LSJh154s8W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentiment_df['Review'] = sentiment_df['Review'].apply(delete_stopwords)"],"metadata":{"id":"i0qdo2SMOhx5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove White spaces\n","sentiment_df['Review'] =sentiment_df['Review'].apply(lambda x: \" \".join(x.split()))"],"metadata":{"id":"EgLJGffy4vm0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 6. Rephrase Text"],"metadata":{"id":"c49ITxTc407N"}},{"cell_type":"code","source":["# Rephrase Text"],"metadata":{"id":"foqY80Qu48N2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 7. Tokenization"],"metadata":{"id":"OeJFEK0N496M"}},{"cell_type":"code","source":["# Tokenization\n","sentiment_df['Review'] = sentiment_df['Review'].apply(nltk.word_tokenize)"],"metadata":{"id":"ijx1rUOS5CUU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 8. Text Normalization"],"metadata":{"id":"9ExmJH0g5HBk"}},{"cell_type":"code","source":["# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n","# Normalizing Text Lemmatization\n","from nltk.stem import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","def lemmatize_tokens(tokens):\n","    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","    return lemmatized_tokens\n","sentiment_df['Review'] = sentiment_df['Review'].apply(lemmatize_tokens)"],"metadata":{"id":"AIJ1a-Zc5PY8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which text normalization technique have you used and why?"],"metadata":{"id":"cJNqERVU536h"}},{"cell_type":"markdown","source":["I have done lemmatization technique as it is good way to reduce words which are used in different ways can be converted into its root words."],"metadata":{"id":"Z9jKVxE06BC1"}},{"cell_type":"markdown","source":["#### 9. Part of speech tagging"],"metadata":{"id":"k5UmGsbsOxih"}},{"cell_type":"code","source":["# POS Taging"],"metadata":{"id":"btT3ZJBAO6Ik"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 10. Text Vectorization"],"metadata":{"id":"T0VqWOYE6DLQ"}},{"cell_type":"code","source":["# Vectorizing Text\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","vectorizer = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)\n","vectorizer.fit(sentiment_df['Review'].values)\n","X_tfidf = vectorizer.transform(sentiment_df['Review'].values)"],"metadata":{"id":"yBRtdhth6JDE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which text vectorization technique have you used and why?"],"metadata":{"id":"qBMux9mC6MCf"}},{"cell_type":"markdown","source":["I utilized TF-IDF (Term Frequency-Inverse Document Frequency) since it can be more effective in certain situations compared to Count Vectorization."],"metadata":{"id":"su2EnbCh6UKQ"}},{"cell_type":"markdown","source":["### 4. Feature Manipulation & Selection"],"metadata":{"id":"-oLEiFgy-5Pf"}},{"cell_type":"markdown","source":["#### 1. Feature Manipulation"],"metadata":{"id":"C74aWNz2AliB"}},{"cell_type":"code","source":["# Manipulate Features to minimize feature correlation and create new features\n","meta_df = meta_df.drop(columns = ['Links'], axis = 1)"],"metadata":{"id":"h1qC4yhBApWC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cluster_dummy.head(1)"],"metadata":{"id":"asDiA-qJRGMm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sentiment_df which is previous post processing output of review_df.\n","# For ratings above average we take as 1 and below average 0.\n","sentiment_df['Sentiment'] = sentiment_df['Rating'].apply(\n","    lambda x: 1 if x >=sentiment_df['Rating'].mean() else 0)"],"metadata":{"id":"Ekuh8MErRLJF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Feature Selection"],"metadata":{"id":"2DejudWSA-a0"}},{"cell_type":"code","source":["# Select your features wisely to avoid overfitting"],"metadata":{"id":"YLhe8UmaBCEE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What all feature selection methods have you used  and why?"],"metadata":{"id":"pEMng2IbBLp7"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"rb2Lh6Z8BgGs"}},{"cell_type":"markdown","source":["##### Which all features you found important and why?"],"metadata":{"id":"rAdphbQ9Bhjc"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"fGgaEstsBnaf"}},{"cell_type":"markdown","source":["### 5. Data Transformation"],"metadata":{"id":"TNVZ9zx19K6k"}},{"cell_type":"markdown","source":["#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"],"metadata":{"id":"nqoHp30x9hH9"}},{"cell_type":"code","source":["\n","from scipy.stats import skew\n","from scipy import stats"],"metadata":{"id":"I6quWQ1T9rtH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["z = np.abs(stats.zscore(cluster_dummy[['Cost']]))\n","print(z)\n","cluster_dummy=cluster_dummy[(z<3).all(axis=1)]\n","cluster_dummy.shape"],"metadata":{"id":"lizj75LQRaId"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Transform Your data\n","symmetric_feature=[]\n","non_symmetric_feature=[]\n","for i in meta_df.describe().columns:\n","  if abs(meta_df[i].mean()-meta_df[i].median())<0.1:\n","    symmetric_feature.append(i)\n","  else:\n","    non_symmetric_feature.append(i)\n","\n","# Getting Symmetric Distributed Features\n","print(\"Symmetric Distributed Features : -\",symmetric_feature)\n","\n","# Getting Skew Symmetric Distributed Features\n","print(\"Skew Symmetric Distributed Features : -\",non_symmetric_feature)"],"metadata":{"id":"qB81aPAzRiOI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cluster_dummy['Cost'] = np.log1p(cluster_dummy['Cost'])"],"metadata":{"id":"X3_HQNJrRprP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["I applied a log transformation to the \"Cost\" column to address its positive skewness."],"metadata":{"id":"ZqnixY_PRu-X"}},{"cell_type":"markdown","source":["### 6. Data Scaling"],"metadata":{"id":"rMDnDkt2B6du"}},{"cell_type":"code","source":["\n","from sklearn.preprocessing import StandardScaler"],"metadata":{"id":"dL9LWpySC6x_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Scaling your data\n","numerical_cols = ['Cost','Total_Cuisine_Count','Average_Rating']\n","scaler = StandardScaler()\n","scaler.fit(cluster_dummy[numerical_cols])\n","scaled_df = cluster_dummy.copy()\n","scaled_df[numerical_cols] = scaler.transform(cluster_dummy[numerical_cols])"],"metadata":{"id":"zlYY5AIkR8UF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which method have you used to scale you data and why?\n","In this case, I applied the Standard Scaler, which is appropriate for data that exhibits a normal distribution."],"metadata":{"id":"yiiVWRdJDDil"}},{"cell_type":"markdown","source":["### 7. Dimesionality Reduction"],"metadata":{"id":"1UUpS68QDMuG"}},{"cell_type":"markdown","source":["##### Do you think that dimensionality reduction is needed? Explain Why?"],"metadata":{"id":"kexQrXU-DjzY"}},{"cell_type":"markdown","source":["Indeed, with a higher number of features, there's a risk of overfitting the model, which can result in poor generalization. Therefore, feature reduction becomes crucial to improve model performance."],"metadata":{"id":"GGRlBsSGDtTQ"}},{"cell_type":"code","source":["\n","scaled_df.head(1)"],"metadata":{"id":"kQfvxBBHDvCa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.decomposition import PCA"],"metadata":{"id":"lmImnkqFSYKb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scaled_df.set_index(['Restaurant'],inplace=True)"],"metadata":{"id":"GKTmGx72SdNz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# DImensionality Reduction (If needed)\n","features = scaled_df.columns\n","pca = PCA()\n","pca.fit(scaled_df[features])"],"metadata":{"id":"wFfGZpzOSi1G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#explained variance v/s no. of components\n","plt.plot(np.cumsum(pca.explained_variance_ratio_), marker ='o', color = 'orange')\n","plt.xlabel('number of components',size = 15, color = 'red')\n","plt.ylabel('cumulative explained variance',size = 14, color = 'blue')\n","plt.title('Variance v/s No. of Components',size = 20, color = 'green')\n","plt.xlim([0, 8])\n","plt.show()"],"metadata":{"id":"FIkHfdQzSpKo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pca = PCA(n_components=3)\n","pca.fit(scaled_df[features])\n","print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n","print('Cumulative variance explained by 3 principal components: {:.2%}'.format(\n","                                        np.sum(pca.explained_variance_ratio_)))\n","df_pca = pca.transform(scaled_df[features])"],"metadata":{"id":"xV4h_-KxSvbr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"],"metadata":{"id":"T5CmagL3EC8N"}},{"cell_type":"markdown","source":["PCA (Principal Component Analysis) is a widely used dimensionality reduction method. In this particular case, PCA was employed to reduce the dimensionality to 3 based on the characteristics of the elbow curve."],"metadata":{"id":"ZKr75IDuEM7t"}},{"cell_type":"markdown","source":["### 8. Data Splitting"],"metadata":{"id":"BhH2vgX9EjGr"}},{"cell_type":"code","source":["# Split your data to train and test. Choose Splitting ratio wisely.\n","X = X_tfidf # I have created this during vectorization\n","y = sentiment_df['Sentiment']"],"metadata":{"id":"0CTyd2UwEyNM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n","\n","# describes info about train and test set\n","print(\"Number transactions X_train dataset: \", X_train.shape)\n","print(\"Number transactions y_train dataset: \", y_train.shape)\n","print(\"Number transactions X_test dataset: \", X_test.shape)\n","print(\"Number transactions y_test dataset: \", y_test.shape)"],"metadata":{"id":"1_dH9dKWTCXG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What data splitting ratio have you used and why?"],"metadata":{"id":"qjKvONjwE8ra"}},{"cell_type":"markdown","source":["\n","Given the dataset's size of approximately 10,000 records, I decided to split the data into an 80:20 ratio for training and testing, respectively."],"metadata":{"id":"Y2lJ8cobFDb_"}},{"cell_type":"markdown","source":["### 9. Handling Imbalanced Dataset"],"metadata":{"id":"P1XJ9OREExlT"}},{"cell_type":"code","source":["sentiment_df.Sentiment.value_counts(normalize = True).mul(100).round(2)"],"metadata":{"id":"q3RN-_1LTVc0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Do you think the dataset is imbalanced? Explain Why."],"metadata":{"id":"VFOzZv6IFROw"}},{"cell_type":"markdown","source":["It appears that the dataset is well-suited for analysis since there is only a minor class imbalance.\n","\n","\n"],"metadata":{"id":"GeKDIv7pFgcC"}},{"cell_type":"code","source":["# Handling Imbalanced Dataset (If needed)"],"metadata":{"id":"nQsRhhZLFiDs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"],"metadata":{"id":"TIqpNgepFxVj"}},{"cell_type":"markdown","source":["Answer Here."],"metadata":{"id":"qbet1HwdGDTz"}},{"cell_type":"markdown","source":["## ***7. ML Model Implementation***"],"metadata":{"id":"VfCC591jGiD4"}},{"cell_type":"markdown","source":["### ML Model - 1"],"metadata":{"id":"OB4l2ZhMeS1U"}},{"cell_type":"code","source":["# ML Model - 1 Implementation\n","from sklearn.cluster import KMeans\n","\n","wcss=[]\n","for i in range(1,11):\n","    km=KMeans(n_clusters=i,random_state = 20)\n","    km.fit(df_pca)\n","    wcss.append(km.inertia_)\n","\n","# Fit the Algorithm\n","\n","# Predict on the model"],"metadata":{"id":"7ebyywQieS1U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Elbow curve\n","plt.plot(range(1,11),wcss)\n","plt.plot(range(1,11),wcss, linewidth=2, color=\"red\", marker =\"o\")\n","plt.xlabel(\"K Value\", size = 20, color = 'purple')\n","plt.xticks(np.arange(1,11,1))\n","plt.ylabel(\"WCSS\", size = 20, color = 'green')\n","plt.title('Elbow Curve', size = 20, color = 'blue')\n","plt.show()"],"metadata":{"id":"8aPz0DP_TphC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#silhouette score\n","import matplotlib.cm as cm\n","from sklearn.metrics import silhouette_score\n","from sklearn.metrics import silhouette_samples\n","from sklearn.model_selection import ParameterGrid\n","# candidates for the number of cluster\n","parameters = list(range(2,10))\n","#parameters\n","parameter_grid = ParameterGrid({'n_clusters': parameters})\n","best_score = -1\n","#visualizing Silhouette Score for individual clusters and the clusters made\n","for n_clusters in parameters:\n","    # Create a subplot with 1 row and 2 columns\n","    fig, (ax1, ax2) = plt.subplots(1, 2)\n","    fig.set_size_inches(18, 7)\n","\n","    # 1st subplot is the silhouette plot\n","    # silhouette coefficient can range from -1, 1 but in this example all\n","    # lie within [-0.1, 1]\n","    ax1.set_xlim([-0.1, 1])\n","    # (n_clusters+1)*10 is for inserting blank space between silhouette\n","    # plots of individual clusters, to demarcate them clearly.\n","    ax1.set_ylim([0, len(df_pca) + (n_clusters + 1) * 10])\n","\n","    # Initialize the clusterer with n_clusters value and a random generator\n","    # seed of 10 for reproducibility.\n","    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n","    cluster_labels = clusterer.fit_predict(df_pca)\n","\n","    # silhouette_score gives the average value for all the samples.\n","    # This gives a perspective into the density and separation of the formed\n","    # clusters\n","    silhouette_avg = silhouette_score(df_pca, cluster_labels)\n","    print(\"For n_clusters =\", n_clusters,\n","          \"average silhouette_score is :\", silhouette_avg)\n","\n","    # Compute the silhouette scores for each sample\n","    sample_silhouette_values = silhouette_samples(df_pca, cluster_labels)\n","\n","    y_lower = 10\n","    for i in range(n_clusters):\n","        # Aggregate the silhouette scores for samples belonging to\n","        # cluster i, and sort them\n","        ith_cluster_silhouette_values = \\\n","            sample_silhouette_values[cluster_labels == i]\n","\n","        ith_cluster_silhouette_values.sort()\n","\n","        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n","        y_upper = y_lower + size_cluster_i\n","\n","        color = cm.nipy_spectral(float(i) / n_clusters)\n","        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n","                          0, ith_cluster_silhouette_values,\n","                          facecolor=color, edgecolor=color, alpha=0.7)\n","\n","        # Label the silhouette plots with their cluster numbers at the middle\n","        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n","\n","        # Compute the new y_lower for next plot\n","        y_lower = y_upper + 10  # 10 for the 0 samples\n","\n","    ax1.set_title(\"silhouette plot for the various clusters.\")\n","    ax1.set_xlabel(\"silhouette coefficient values\")\n","    ax1.set_ylabel(\"Cluster label\")\n","\n","    # vertical line for average silhouette score of all the values\n","    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n","\n","    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n","    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n","\n","    # 2nd Plot showing the actual clusters formed\n","    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n","    ax2.scatter(df_pca[:, 0], df_pca[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n","                c=colors, edgecolor='k')\n","\n","    # Labeling the clusters\n","    centers = clusterer.cluster_centers_\n","    # Draw white circles at cluster centers\n","    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n","                c=\"white\", alpha=1, s=200, edgecolor='k')\n","    #marker='$%d$' % i will give numer in cluster in 2 plot\n","    for i, c in enumerate(centers):\n","        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n","                    s=50, edgecolor='k')\n","\n","    ax2.set_title(\"visualization of the clustered data.\")\n","    ax2.set_xlabel(\"Feature space for the 1st feature\")\n","    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n","    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n","                  \"with n_clusters = %d\" % n_clusters),\n","                 fontsize=14, fontweight='bold')"],"metadata":{"id":"i5Aq4gGnT1tN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#vizualizing the clusters and the datapoints in each clusters\n","plt.figure(figsize = (10,6), dpi = 120)\n","\n","kmeans= KMeans(n_clusters = 6, init= 'k-means++', random_state = 42)\n","kmeans.fit(df_pca)\n","\n","#predict the labels of clusters.\n","label = kmeans.fit_predict(df_pca)\n","#Getting unique labels\n","unique_labels = np.unique(label)\n","\n","#plotting the results:\n","for i in unique_labels:\n","    plt.scatter(df_pca[label == i , 0] , df_pca[label == i , 1] , label = i)\n","plt.legend()\n","plt.show()"],"metadata":{"id":"Yjm0K8LSUHYP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["kmeans_pca_df = pd.DataFrame(df_pca,columns=['PC1','PC2','PC3'],index=scaled_df.index)\n","kmeans_pca_df[\"label\"] = label\n","kmeans_pca_df.sample(2)"],"metadata":{"id":"6uOWJZBmUMwg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# joining clusters\n","cluster_dummy.set_index(['Restaurant'],inplace=True)\n","cluster_dummy = cluster_dummy.join(kmeans_pca_df['label'])\n","cluster_dummy.sample(2)"],"metadata":{"id":"JIJ5PNc6USml"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# back to normal from log during transformation\n","cluster_dummy['Cost'] = np.expm1(cluster_dummy['Cost'])\n","cluster_dummy.sample(2)"],"metadata":{"id":"nJTRCeF9UXmD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clustering_result = cluster_dummy.copy().reset_index()\n","clustering_result = meta_df[['Restaurant','Cuisines']].merge(clustering_result[['Restaurant','Cost',\n","                  'Average_Rating',\t'Total_Cuisine_Count','label']], on = 'Restaurant')\n","clustering_result.head()"],"metadata":{"id":"_8HxRpG7Ucu2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# counting cluster observations\n","cluster_count = cluster_dummy['label'].value_counts().reset_index().rename(\n","    columns={'index':'label','label':'Total_Restaurant'}).sort_values(by='Total_Restaurant')\n","cluster_count"],"metadata":{"id":"JuDohCpVUjND"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_cluster_df = clustering_result.copy()\n","new_cluster_df['Cuisines'] = new_cluster_df['Cuisines'].str.split(',')\n","new_cluster_df = new_cluster_df.explode('Cuisines')\n","#removing extra trailing space from cuisines after exploded\n","new_cluster_df['Cuisines'] = new_cluster_df['Cuisines'].apply(lambda x: x.strip())\n","new_cluster_df.sample(5)"],"metadata":{"id":"vOtKZnWRUn-w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#printing cuisine list for each cluster\n","for cluster in new_cluster_df['label'].unique().tolist():\n","  print('Cuisine List for Cluster :', cluster,'\\n')\n","  print(new_cluster_df[new_cluster_df[\"label\"]== cluster]['Cuisines'].unique(),'\\n')\n","  print('='*120)"],"metadata":{"id":"F7R5kKM-Uuwo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["I opted for the K-means clustering method in this case because the data doesn't have hierarchical categories, making K-means suitable. Additionally, I determined that K = 6 is the optimal choice, as it yields the highest silhouette score."],"metadata":{"id":"3fYkYtdsU7Ep"}},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"ArJBuiUVfxKd"}},{"cell_type":"code","source":["# Visualizing evaluation Metric Score chart"],"metadata":{"id":"rqD5ZohzfxKe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"4qY1EAkEfxKe"}},{"cell_type":"code","source":["# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","\n","# Fit the Algorithm\n","\n","# Predict on the model"],"metadata":{"id":"Dy61ujd6fxKe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"PiV4Ypx8fxKe"}},{"cell_type":"markdown","source":["I employed a combination of the elbow curve and silhouette score analysis to determine the optimal number of clusters (K) in K-means clustering. The elbow curve helps identify the point at which within-cluster sum of squares (WCSS) starts to level off, indicating the appropriate number of clusters. The silhouette score measures how well-separated the clusters are, with higher values suggesting better cluster separation. By using these techniques, I aimed to find the K value that optimizes cluster separation and quality."],"metadata":{"id":"negyGRa7fxKf"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"TfvqoZmBfxKf"}},{"cell_type":"markdown","source":["Yes, the key improvement observed is in the selection of an appropriate value for K. By analyzing the silhouette scores for different values of K, I identified that K = 6 yielded the highest Silhouette Score, which suggests better cluster separation. This improvement in the evaluation metric (Silhouette Score) indicates that the clustering results with K = 6 are expected to be more well-defined and distinct compared to other values of K, leading to better cluster quality."],"metadata":{"id":"OaLui8CcfxKf"}},{"cell_type":"markdown","source":["### ML Model - 2"],"metadata":{"id":"dJ2tPlVmpsJ0"}},{"cell_type":"code","source":["!pip install lda"],"metadata":{"id":"5R1Sb5K5VP1M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import silhouette_score\n","from sklearn.decomposition import LatentDirichletAllocation\n","\n","topic_range = range(2,11)\n","silhouette_score =[]\n","\n","for n_components in topic_range:\n","    lda = LatentDirichletAllocation(n_components=6)\n","    lda.fit(X)\n","    labels = lda.transform(X).argmax(axis=1)\n","    silhouette_scores.append(silhouette_score(X, labels))"],"metadata":{"id":"Wn5OnBMMWz3o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#plotting silhouette score\n","plt.plot(topic_range, silhouette_score, marker ='o', color='red')\n","plt.xlabel('Number of Topics', size = 15, color = 'green')\n","plt.ylabel('Silhouette Score', size = 15, color = 'blue')\n","plt.show()"],"metadata":{"id":"7H6-tEi0X7iJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LDA model\n","lda = LatentDirichletAllocation(n_components=4)\n","lda.fit(X)"],"metadata":{"id":"AU-ZPpzLYc5y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install pyLDAvis"],"metadata":{"id":"-xiI-WA7YpdC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pyLDAvis\n","import pyLDAvis.gensim_models\n","pyLDAvis.enable_notebook(local=True)"],"metadata":{"id":"yjJPOlqTYwT7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["topic_term_dists = lda.components_ / lda.components_.sum(axis=1)[:,None]\n","doc_lengths = X_tfidf.sum(axis=1).getA1()\n","term_frequency = X_tfidf.sum(axis=0).getA1()\n","lda_doc_topic_dists = lda.transform(X)\n","doc_topic_dists = lda_doc_topic_dists / lda_doc_topic_dists.sum(axis=1)[:,None]\n","vocab = vectorizer.get_feature_names_out()"],"metadata":{"id":"U1zHlsx1Y18F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ploting the clusters top 30 terms\n","# lda_pyLDAvis = pyLDAvis.prepare(lda, X, vectorizer)\n","a = pyLDAvis.prepare(topic_term_dists,doc_topic_dists,doc_lengths,vocab,term_frequency)\n","pyLDAvis.display(a)"],"metadata":{"id":"_C-_FReQZAPa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["review_sentiment_prediction = review_df[review_df.columns.to_list()].copy()\n","topic_results = lda.transform(X)\n","review_sentiment_prediction['Prediction'] = topic_results.argmax(axis=1)\n","review_sentiment_prediction.sample(5)"],"metadata":{"id":"b-3WX9WZZL0F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from wordcloud import WordCloud\n","# Define the number of words to include in the word cloud\n","N = 100\n","\n","# Create a list of strings for each topic\n","topic_text = []\n","for index, topic in enumerate(lda.components_):\n","    topic_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-N:]]\n","    topic_text.append(\" \".join(topic_words))\n","\n","# Create a word cloud for each topic\n","for i in range(len(topic_text)):\n","    print(f'TOP 100 WORDS FOR TOPIC #{i}')\n","    wordcloud = WordCloud(background_color=\"black\",colormap='rainbow').generate(topic_text[i])\n","    plt.figure(figsize=(10,5))\n","    plt.imshow(wordcloud, interpolation='bilinear')\n","    plt.axis(\"off\")\n","    plt.show()\n","    print('='*120)"],"metadata":{"id":"P-vr3PSKZLw1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for sentiment in review_sentiment_prediction['Prediction'].unique().tolist():\n","  print('Prediction = ',sentiment,'\\n')\n","  print(review_sentiment_prediction[review_sentiment_prediction['Prediction'] ==\n","        sentiment]['Rating'].value_counts())\n","  print('='*120)"],"metadata":{"id":"PAgZZs38Zij6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#defining function to calculate score\n","from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n","from tabulate import tabulate\n","import itertools\n","\n","\n","#calculating score\n","def calculate_scores(model, X_train, y_train, X_test, y_test):\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","    roc_auc = roc_auc_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred)\n","    recall = recall_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred)\n","    accuracy = accuracy_score(y_test, y_pred)\n","    # Get the confusion matrix for both train and test\n","\n","    cm = confusion_matrix(y_test, y_pred)\n","    plt.imshow(cm, cmap='Wistia')\n","\n","    # Add labels to the plot\n","    class_names = [\"Positive\", \"Negative\"]\n","    tick_marks = np.arange(len(class_names))\n","    plt.xticks(tick_marks, class_names)\n","    plt.yticks(tick_marks, class_names)\n","\n","    # Add values inside the confusion matrix\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, cm[i, j],\n","                horizontalalignment=\"center\",\n","                color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    # Add a title and x and y labels\n","    plt.title('Confusion Matrix')\n","    plt.xlabel('Predicted label')\n","    plt.ylabel('True label')\n","\n","    plt.show()\n","    print(cm)\n","    return roc_auc, f1, accuracy, precision, recall\n","\n","#printing result\n","def print_table(model, X_train, y_train, X_test, y_test):\n","    roc_auc, f1, accuracy, precision, recall = calculate_scores(model, X_train, y_train, X_test, y_test)\n","    table = [[\"ROC AUC\", roc_auc], [\"Precision\", precision],\n","             [\"Recall\", recall], [\"F1\", f1], [\"Accuracy\", accuracy]]\n","    print(tabulate(table, headers=[\"Metric\", \"Score\"]))"],"metadata":{"id":"8QQOF__dZn8v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Logistic and XGBoost algorithms**\n","\n"],"metadata":{"id":"ZAHCelyudCPB"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from xgboost import XGBClassifier\n","clf = LogisticRegression()\n","xgb = XGBClassifier()"],"metadata":{"id":"33g-OblsdJvB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# logitic regression GridSearchCV\n","param_dict = {'C': [0.1,1,10,100,1000],'penalty': ['l1', 'l2'],'max_iter':[1000]}\n","clf_grid = GridSearchCV(clf, param_dict,n_jobs=-1, cv=5, verbose = 5,scoring='recall')\n","clf_grid.fit(X_train,y_train)\n","# XGBoost gridsearchCV\n","xgb_param={'n_estimators': [100,125,150],'max_depth': [7,10,15],'criterion': ['entropy']}\n","xgb_grid=GridSearchCV(estimator=xgb,param_grid = xgb_param,cv=3,scoring='recall',verbose=5,n_jobs = -1)\n","xgb_grid.fit(X_train,y_train)"],"metadata":{"id":"FEcwFUTIdTSj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Plotting graph\n","from sklearn.metrics import roc_curve\n","# finding the best parameters for all the models\n","log_reg_best = clf_grid.best_estimator_\n","xgbc_best = xgb_grid.best_estimator_\n","\n","# predicting the sentiment by all models\n","y_preds_proba_lr = log_reg_best.predict_proba(X_test)[::,1]\n","y_preds_proba_xgbc = xgbc_best.predict_proba(X_test)[::,1]\n","\n","classifiers_proba = [(log_reg_best, y_preds_proba_lr),\n","                    (xgbc_best, y_preds_proba_xgbc)]\n","\n","# Define a result table as a DataFrame\n","result_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n","\n","# Train the models and record the results\n","for pair in classifiers_proba:\n","\n","    fpr, tpr, _ = roc_curve(y_test,  pair[1])\n","    auc = roc_auc_score(y_test, pair[1])\n","\n","    result_table = result_table._append({'classifiers':pair[0].__class__.__name__,\n","                                        'fpr':fpr,\n","                                        'tpr':tpr,\n","                                        'auc':auc}, ignore_index=True)\n","\n","# Set name of the classifiers as index labels\n","result_table.set_index('classifiers', inplace=True)\n","\n","# ploting the roc auc curve for all models\n","fig = plt.figure(figsize=(10,6))\n","\n","for i in result_table.index:\n","    plt.plot(result_table.loc[i]['fpr'],\n","             result_table.loc[i]['tpr'],\n","             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n","\n","plt.plot([0,1], [0,1],'r--')\n","\n","plt.xlabel(\"False Positive Rate\", fontsize=15)\n","\n","plt.ylabel(\"True Positive Rate\", fontsize=15)\n","\n","plt.title('ROC AUC Curve', fontweight='bold', fontsize=15)\n","plt.legend(prop={'size':13}, loc='lower right')\n","\n","plt.show()"],"metadata":{"id":"n19LfTSaeM9J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"-jK_YjpMpsJ2"}},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"HAih1iBOpsJ2"}},{"cell_type":"markdown","source":["I employed GridSearch, even though it can be time-consuming, because it's a powerful optimization technique that typically leads to the best outcomes."],"metadata":{"id":"9kBgjYcdpsJ2"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"zVGeBEFhpsJ2"}},{"cell_type":"markdown","source":["\n","The method has produced highly favorable results, indicating its effectiveness."],"metadata":{"id":"74yRdG6UpsJ3"}},{"cell_type":"markdown","source":["#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."],"metadata":{"id":"bmKjuQ-FpsJ3"}},{"cell_type":"markdown","source":["The ROC AUC score quantifies the probability of accurately classifying new observations, making it of significant business importance."],"metadata":{"id":"BDKtOrBQpsJ3"}},{"cell_type":"markdown","source":["## ***8.*** ***Future Work (Optional)***"],"metadata":{"id":"EyNgTHvd2WFk"}},{"cell_type":"markdown","source":["### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"],"metadata":{"id":"KH5McJBi2d8v"}},{"cell_type":"code","source":["# Save the File"],"metadata":{"id":"bQIANRl32f4J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"],"metadata":{"id":"iW_Lq9qf2h6X"}},{"cell_type":"code","source":["# Load the File and predict unseen data."],"metadata":{"id":"oEXk9ydD2nVC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"],"metadata":{"id":"-Kee-DAl2viO"}},{"cell_type":"markdown","source":["# **Conclusion**"],"metadata":{"id":"gCX9965dhzqZ"}},{"cell_type":"markdown","source":["In this project, we successfully applied various data preprocessing techniques to enhance the quality of our textual dataset, making it suitable for machine learning.\n","\n","Optimized Clustering: Through K-means clustering, we optimized the number of clusters (K) and improved the Silhouette Score, indicating better cluster separation, with K=6 as the optimal value.\n","\n","Topic Modeling: We employed Latent Dirichlet Allocation (LDA) to reveal key topics within reviews. The topic analysis provided valuable insights into the most discussed themes.\n","\n","Model Performance: Supervised models, including Logistic Regression and XGBoost, saw remarkable improvements in key evaluation metrics after hyperparameter tuning.\n","\n","Improved Precision: Achieved high precision, essential for accurate positive sentiment prediction in sentiment analysis tasks, which is crucial for businesses interpreting customer sentiment.\n","\n","Enhanced Recall: Demonstrated high recall, ensuring that positive instances are captured effectively, a valuable feature in applications that should not miss positive cases.\n","\n","Balanced F1-Score: Attained a balanced F1-score, offering a trade-off between precision and recall, which is beneficial for applications demanding a balance between accuracy and coverage.\n","\n","ROC AUC Significance: The ROC AUC score improved post-tuning, highlighting the model's ability to distinguish between positive and negative instances, pivotal for classification accuracy.\n","\n","Model Deployment: Discussed the importance of saving the best-performing model in a deployable format, setting the stage for real-world applications.\n","\n","Future Prospects: Highlighted the potential for live server deployment and the significance of evaluating the model on unseen data for real-world sanity checks.\n","\n","Impact on Business: Emphasized that our project has the potential to empower businesses with data-driven insights, enhancing decision-making capabilities.\n","\n","Data Transformation: From text preprocessing to model selection, our project showcases the transformative power of data science, converting raw data into actionable knowledge."],"metadata":{"id":"Fjb1IsQkh3yE"}},{"cell_type":"markdown","source":["### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"],"metadata":{"id":"gIfDvo9L0UH2"}}]}